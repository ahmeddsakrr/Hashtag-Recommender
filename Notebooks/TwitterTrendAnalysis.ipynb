{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Twitter API Access\n",
    "\n",
    "In order to use it to make requests to Twitter's API, you'll need to go to https://dev.twitter.com/apps and create a sample application.\n",
    "\n",
    "Under **Key and Access Tokens**, there are four primary identifiers you'll need to note:\n",
    "* consumer key,\n",
    "* consumer secret,\n",
    "* access token, and\n",
    "* access token secret (Click on Create Access Token to create those).\n",
    "\n",
    "Note that you will need an ordinary Twitter account in order to login, create an app, and get these credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T17:46:54.400559Z",
     "start_time": "2023-05-01T17:46:53.941586Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Libraries Needed\n",
    "import tweepy\n",
    "import csv\n",
    "import pandas as pd\n",
    "import re\n",
    "import preprocessor as p\n",
    "import nltk\n",
    "import string\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "#A stop word is a commonly used word (such as “the”, “a”, “an”, “in”)\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Authorizing an application to access Twitter account data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T17:46:54.444559Z",
     "start_time": "2023-05-01T17:46:54.401560Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_dotenv() # loads environment variables from .env file\n",
    "#API_key\n",
    "a_k = os.environ.get('TWITTER_API_KEY')\n",
    "\n",
    "#API_Secret_Key\n",
    "a_sk = os.environ.get('TWITTER_API_SECRET_KEY')\n",
    "\n",
    "#Access_Token\n",
    "a_t = os.environ.get('TWITTER_ACCESS_TOKEN')\n",
    "\n",
    "#Access_Token_Secret\n",
    "a_s = os.environ.get('TWITTER_ACCESS_TOKEN_SECRET')\n",
    "\n",
    "auth = tweepy.OAuthHandler(a_k, a_sk)\n",
    "auth.set_access_token(a_t, a_s)\n",
    "api = tweepy.API(auth , wait_on_rate_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Creating the Ouput file and specifying the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T17:46:54.474589Z",
     "start_time": "2023-05-01T17:46:54.420559Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#New Files\n",
    "USFile = r\"..\\NewData\\USTweets.csv\"\n",
    "UKFile = r\"..\\NewData\\UKTweets.csv\"\n",
    "AUSFile = r\"..\\NewData\\AUSTweets.csv\"\n",
    "CANFile = r\"..\\NewData\\CANTweets.csv\"\n",
    "IRFile = r\"..\\NewData\\IRTweets.csv\"\n",
    "#Total Files\n",
    "tUSFile = r\"..\\Data\\USTweets.csv\"\n",
    "tUKFile = r\"..\\Data\\UKTweets.csv\"\n",
    "tAUSFile = r\"..\\Data\\AUSTweets.csv\"\n",
    "tCANFile = r\"..\\Data\\CANTweets.csv\"\n",
    "tIRFile = r\"..\\Data\\IRTweets.csv\"\n",
    "#columns of the csv file\n",
    "COLS = ['id', 'created_at', 'source', 'original_text','clean_text','favorite_count',\n",
    "        'retweet_count', 'hashtags','trend']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Resetting the New Files to include only New Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T17:46:54.584557Z",
     "start_time": "2023-05-01T17:46:54.435562Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for old in [USFile , UKFile , AUSFile , CANFile , IRFile ]:\n",
    "    df = pd.read_csv(old, header=None)\n",
    "    df.head(1).to_csv(old, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Handling unwanted characters in the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T17:46:54.599561Z",
     "start_time": "2023-05-01T17:46:54.590559Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#handle emoticons and emojis\n",
    "\n",
    "happyEmoticons = {':-)', ':)', ';)', ':o)', ':]', ':3', ':c)', ':>', '=]', '8)', '=)', ':}', ':^)', ':-D', ':D', '8-D',\n",
    "                   '8D', 'x-D', 'xD', 'X-D', 'XD', '=-D', '=D', '=-3', '=3', ':-))', \":'-)\", \":')\", ':*', ':^*', '>:P',\n",
    "                   ':-P', ':P', 'X-P', 'x-p', 'xp', 'XP', ':-p', ':p', '=p', ':-b', ':b', '>:)', '>;)', '>:-)', '<3'}\n",
    "\n",
    "sadEmoticons = {':L', ':-/', '>:/', ':S', '>:[', ':@', ':-(', ':[', ':-||', '=L', ':<', ':-[', ':-<', '=\\\\', '=/',\n",
    "                 '>:(', ':(', '>.<', \":'-(\", \":'(\", ':\\\\', ':-c', ':c', ':{', '>:\\\\', ';('}\n",
    "\n",
    "emoticons = happyEmoticons.union(sadEmoticons)\n",
    "\n",
    "\n",
    "emojis = re.compile(\"[\"\n",
    "         u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "         u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "         u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "         u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "         u\"\\U00002702-\\U000027B0\"\n",
    "         u\"\\U000024C2-\\U0001F251\"\n",
    "         \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Further Cleaning for the Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T17:46:54.628557Z",
     "start_time": "2023-05-01T17:46:54.603558Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_tweets(tweet):\n",
    "#after tweepy preprocessing the colon symbol still exists after removing mentions\n",
    "    tweet = re.sub(r':', '', tweet)\n",
    "#Unhandled symbols to be removed\n",
    "    tweet = re.sub(r'‚Ä¶', '', tweet)\n",
    "#replace consecutive non-ASCII characters with a space\n",
    "    tweet = re.sub(r'[^\\x00-\\x7F]+',' ', tweet)\n",
    "#remove emojis from tweet\n",
    "    tweet = emojis.sub(r'', tweet)\n",
    "#removing hashtag symbol\n",
    "    tweet = re.sub(\"#\" ,\"\" , tweet )\n",
    "#removing quotation marks\n",
    "    tweet = re.sub('[\\'\\\"]', '', tweet)\n",
    "#removing numbers\n",
    "    tweet = re.sub('[0-9]+','' , tweet)\n",
    "\n",
    "    word_tokens = nltk.word_tokenize(tweet)\n",
    "#filter using NLTK library append it to a string\n",
    "    filtered_tweet = [w for w in word_tokens if not w in stop_words \\\n",
    "                      and w not in emoticons and w not in string.punctuation]\n",
    "    #convert list to string , space separated.\n",
    "    return ' '.join(filtered_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Retrieving Tweets and Saving the Output in the CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T17:46:54.632562Z",
     "start_time": "2023-05-01T17:46:54.623561Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def save_tweets(topic, totalFile, newFile):\n",
    "    header = not os.path.exists(totalFile)\n",
    "    df_final = pd.DataFrame(columns=COLS)\n",
    "    for tweet in tweepy.Cursor(api.search_tweets, q=topic, lang='en', count=5000).items(5000):\n",
    "        statuses = [tweet]\n",
    "        for status in statuses:\n",
    "            parsedData = []\n",
    "            clean_text = p.clean(status.text)\n",
    "            filtered_tweet = clean_tweets(str(clean_text).lower())\n",
    "            if not len(filtered_tweet):\n",
    "                continue\n",
    "            parsedData += [status.id, status.created_at,\n",
    "              status.source, status.text, filtered_tweet,\n",
    "              status.favorite_count, status.retweet_count]\n",
    "            hashtags = \", \".join([hashtag_item['text'] for hashtag_item in status.entities['hashtags']])\n",
    "            hashtags = re.sub('#','',hashtags)\n",
    "            topic = re.sub('#','',topic)\n",
    "            parsedData.append(hashtags) #append the hashtags\n",
    "            parsedData.append(topic) #append the trend allocated to each tweet\n",
    "            single_tweet_df = pd.DataFrame([parsedData], columns=COLS)\n",
    "            df_final = pd.concat([df_final, single_tweet_df], ignore_index=True)\n",
    "    df_final.drop_duplicates(subset=['clean_text'], inplace=True)\n",
    "    df_final.to_csv(totalFile, columns=COLS, mode='a', index=False, header=header, encoding='utf-8')\n",
    "    df_final.to_csv(newFile , columns=COLS , mode='a' , index = False , header = False , encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Getting Trending Topics according to the Place ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T17:46:54.647589Z",
     "start_time": "2023-05-01T17:46:54.636558Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_trends_and_tweets(place_id , id):\n",
    "    trends = api.get_place_trends(id = place_id)\n",
    "    for i in range(10):\n",
    "       if id == 1:\n",
    "           save_tweets(trends[0]['trends'][i]['name'] , tUSFile , USFile)\n",
    "       elif id == 2:\n",
    "           save_tweets(trends[0]['trends'][i]['name'] , tUKFile , UKFile)\n",
    "       elif id == 3:\n",
    "           save_tweets(trends[0]['trends'][i]['name'] , tCANFile , CANFile)\n",
    "       elif id == 4:\n",
    "           save_tweets(trends[0]['trends'][i]['name'] , tAUSFile , AUSFile)\n",
    "       elif id == 5:\n",
    "           save_tweets(trends[0]['trends'][i]['name'] , tIRFile , IRFile)\n",
    "\n",
    "    # for value in trends:\n",
    "    #     for trend in value['trends']:\n",
    "    #         if id == 1:\n",
    "    #             save_tweets(trend['name'] , USFile)\n",
    "    #         elif id == 2:\n",
    "    #             save_tweets(trend['name'] , UKFile)\n",
    "    #         elif id == 3:\n",
    "    #             save_tweets(trend['name'] , CANFile)\n",
    "    #         elif id == 4:\n",
    "    #             save_tweets(trend['name'] , AUSFile)\n",
    "    #         elif id == 5:\n",
    "    #             save_tweets(trend['name'] , IRFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Twitter identifies locations using the Yahoo! Where On Earth ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T22:35:07.927982Z",
     "start_time": "2023-05-01T17:46:54.658559Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "US_WOE_ID = 23424977\n",
    "UK_WOE_ID = 23424975\n",
    "Canada_WOE_ID = 23424775\n",
    "AUS_WOE_ID = 23424748\n",
    "IR_WOE_ID = 23424803\n",
    "\n",
    "get_trends_and_tweets(US_WOE_ID , 1)\n",
    "get_trends_and_tweets(UK_WOE_ID , 2)\n",
    "get_trends_and_tweets(Canada_WOE_ID , 3)\n",
    "get_trends_and_tweets(AUS_WOE_ID , 4)\n",
    "get_trends_and_tweets(IR_WOE_ID , 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Removing Duplicates and Shuffling Rows in CSV File\n",
    "When we want to scrap more tweets in the csv file , this could result in adding tweets we already got days ago."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-01T22:35:16.794824Z",
     "start_time": "2023-05-01T22:35:07.939984Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = [USFile, UKFile, CANFile, AUSFile, IRFile ,tIRFile ,\n",
    "         tUKFile , tUSFile , tCANFile , tAUSFile]\n",
    "for file in files:\n",
    "    df = pd.read_csv(file)\n",
    "    df.drop_duplicates(subset=['clean_text'], inplace=True)\n",
    "    df = pd.concat([df[:1], df[1:].sample(frac=1)]).reset_index(drop=True)\n",
    "    df.to_csv(file, header=True, index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
